{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess, balance, split and save data for use in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from the csv, using pandas DataFrames for all further steps here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "raw_csv_data = pd.read_csv('Audiobooks_data.csv', header=None)\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data.drop(raw_csv_data.columns[[0, -1]], axis=1)\n",
    "targets_all = raw_csv_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Pre)Shuffle the data\n",
    "optional, in this case preshuffling lowers the final accuracy by ~10% because the data seems to be arranged in a way that offers additional patterns. (most likely the way non-buying customers are sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = np.random.permutation(unscaled_inputs_all.index)\n",
    "\n",
    "#unscaled_inputs_all = unscaled_inputs_all.reindex(idx).reset_index(drop=True)\n",
    "#targets_all = targets_all.reindex(idx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset\n",
    "the dataset has by far more 0s than 1s, so we take all 1s and drop 0s until we have the same amount of 0s to balance it to 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))  # gives amount of 1s in dataset (since there are only 0s and 1s)\n",
    "zero_targets_counter = 0  # keep track of how many 0s we already iterated over\n",
    "indices_to_remove = []  # 'mark' all 0s over the amount of 1s as deletable\n",
    "\n",
    "# loop that puts all indices of rows in a list after the amount of 0s equals to that of 1s\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# drop every row which index is in the generated list\n",
    "unscaled_inputs_equal_priors = unscaled_inputs_all.drop(unscaled_inputs_all.index[indices_to_remove], axis=0)\n",
    "targets_equal_priors = targets_all.drop(targets_all.index[indices_to_remove], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the equal-priors-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring list with randomized order of indices\n",
    "idx = np.random.permutation(unscaled_inputs_equal_priors.index)\n",
    "\n",
    "# applying the list of random indices to both dataframes, so they have equal, random shuffled indices\n",
    "unscaled_inputs_equal_priors = unscaled_inputs_equal_priors.reindex(idx).reset_index(drop=True)\n",
    "targets_equal_priors = targets_equal_priors.reindex(idx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiobook_scaler = StandardScaler()\n",
    "audiobook_scaler.fit(unscaled_inputs_equal_priors)\n",
    "# use the sklearn preprocessing function, returns nd-array\n",
    "scaled_inputs = audiobook_scaler.transform(unscaled_inputs_equal_priors)\n",
    "# transform nd-array back into a pd.DataFrame\n",
    "scaled_inputs = pd.DataFrame(scaled_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773 3579 0.4953897736797988\n",
      "234 447 0.5234899328859061\n",
      "230 448 0.5133928571428571\n"
     ]
    }
   ],
   "source": [
    "# get number of all samples\n",
    "samples_count = scaled_inputs.shape[0]\n",
    "\n",
    "# declare variables for the counts off all subset-samples\n",
    "train_samples_count = int(0.8*samples_count)  # 80% of all\n",
    "validation_samples_count = int(0.1*samples_count)  # 10% of all\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count  # rest\n",
    "\n",
    "# actually slicing the dataframes by the set percentages\n",
    "# train-set\n",
    "train_inputs = scaled_inputs[:train_samples_count]\n",
    "train_targets = targets_equal_priors[:train_samples_count]\n",
    "\n",
    "# val-set\n",
    "validation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = targets_equal_priors[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# test-set\n",
    "test_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = targets_equal_priors[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# print: number of 1s (from 1s and 0s), number of samples per set, percentage of split(should be 1 / categories roughly)\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz\n",
    "save the 3 subsets as tuples with tags: 'inputs' for inputs and 'targets' for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Scaler-Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pickle', 'wb') as f:\n",
    "    pickle.dump(audiobook_scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
